---
title: "Stage: Projet 1"
title-block-banner: true
subtitle: "House Prices - Advanced Regression Techniques"
date: last-modified
author: 
  - name: Tra Bi Nene Othniel
  - email: bineneothniel.tra@linkpact.fr
  - url: 
keywords: 
  - R for data-science
  - regression techniques
  - machine learning
format: 
  html:
    
    toc: true
    toc-depth: 3
    toc-expand: 1
    toc-location: left
    toc-title: "table des matières"
    grid:
      sidebar-width: 230px
      body-width: 950px
      margin-width: 100px
      gutter-width: 1.5em
    number-sections: true
    number-depth: 3
    smooth-scroll: true
    code-fold: false
    code-block-border-left: true
    code-summary: "montrer"
    html-math-method: katex
    anchor-sections: false
    other-links:
        
      - icon: box-arrow-up-right
        text: kaggle
        href: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques
theme:
   
    light: lumen
    dark: cyborg
      
editor: visual
column: page-inset-right
anchor-sections: true
smooth-scroll: true
html-math-method: katex
code-annotations: hover
code-link: true
lightbox: true
css: style.css
echo: false
execute: 
  freeze: auto
  error: false
  warning: false
eval: true
comments:
  hypothesis: 
    theme: clean
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
#| file: fonctions.R

library(ggridges) 
library(DataExplorer)
library(naniar)
library(tidyverse)
library(gt)
library(ggplot2)
library(labelled)

library(xgboost)

```

```{r}
train <- read.csv("data/train.csv", na.strings = "NA")
test <- read.csv("data/test.csv", na.strings = "NA")


Y <- train$SalePrice
label <- train$SalePrice
Id <- test$Id

a <- train %>% 
  dplyr::select(-SalePrice) %>%
  mutate( type = "train")

b <- test %>%
  mutate( type = "test")


data <- bind_rows(a, b)
```

# Importation des données

```{r}
#| code-fold: show 

skimr::skim(data)

```

```{r}
train %>%
  slice_head(n = 3) %>%
  gt_trabi() %>%
  tab_source_note(md("Source : kaggle"))

```

-   **Reading the text file line by line**

``` r
lines <- readLines("data/data_description.txt")
cat(lines, sep = "\n")
```

::: {.callout-caution title="basic information about train data" collapse="true"}
```{r}
data_information(train)

```
:::

::: {.callout-caution title="basic information about test data" collapse="true"}
```{r}
data_information(test)

```
:::

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-cap: "missing"
#| fig-subcap: 
#|   - "train"
#|   - "test"

missing_variables(train)
missing_variables(test)
```

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-cap: "missing"
#| fig-subcap: 
#|   - "train"
#|   - "test"

missing_loliplot(train)
missing_loliplot(test)
```

```{r}
#| fig-cap: "missing"
#| fig-subcap: 
#|   - "train"
#|   - "test"
#| layout-nrow: 2
#| layout-ncol: 2

missing_heatmap(train)
missing_heatmap(test)
```

on observe que certaines variables ont des valeurs manquantes qui sont liées

-   **79 variables explicatives**

-   **répartition des types de variables**

```{r}
#| fig-cap: "type de variable"
#| fig-subcap: 
#|   - "train"
#|   - "test"
#| layout-nrow: 2
#| layout-ncol: 2

var_type_viz_1(train)
var_type_viz_1(test)
```

# exploration des données via l'application shiny

# Visualisation des données

## Variable cible (SalePrice)

```{r}
summary(train$SalePrice)
```

```{r}

var_quanti_viz_3(data = train, SalePrice)
```

```{r}
# Segmenter la variable SalePrice en intervalles de 1e+5 
train$sales_cut <- cut(train$SalePrice, breaks = seq(0, max(train$SalePrice, na.rm = TRUE) + 1e+5, by = 1e+5), labels = FALSE)*1e+5

train$sales_cut <- as.factor(train$sales_cut)
ggplot(data = train, mapping = aes( x = sales_cut))+
  geom_bar()

train <- train %>% select(- sales_cut)

```

```{r}
var_quanti_viz_2(data = train, var = SalePrice)
```

## exemple de Variables qualitatives

### MSSubClass

```{r}
#| echo: false
a <- train %>% 
  distinct(MSSubClass) %>%
  nrow()
print(paste("il y a", a, "modalités"))
```

#### distribution

```{r}
train$MSSubClass <- as.character(train$MSSubClass)

var_quali_viz_2(data = train, var = MSSubClass)
```

#### lien avec la variable cible

```{r}
#| column: screen-inset-shaded
#| layout:  "[[1,1], [1]]"

var_cible_quali_1(data = train, var = MSSubClass)

var_cible_quali_3(data = train, var = MSSubClass)

var_cible_quali_2(data = train, var = MSSubClass)

```

## exemple de Variables quantitatives

### LotFrontage

```{r}
summary(train$LotArea)
```

```{r}
missing_viz_1(data = train, class = LotFrontage)
```

```{r}

var_quanti_viz_3(data = train, LotFrontage)
```

```{r}
var_quanti_viz_1(data = train, var = LotFrontage)
```

```{r}
var_quanti_viz_2(data = train, var = LotFrontage)
```

```{r}
var_cible_quanti(data = train, var = LotFrontage)
```

# model

-   **Surface habitable (GrLivArea)** : La taille de l'espace de vie au-dessus du sol.

-   **Nombre de chambres (BedroomAbvGr)** : Le nombre de chambres au-dessus du sol.

-   **Nombre de salles de bains (FullBath, HalfBath)** : Le nombre de salles de bains complètes et demi-salles de bains.

-   **Année de construction (YearBuilt)** : L'année de construction de la maison.

-   **Année de rénovation (YearRemodAdd)** : L'année de la dernière rénovation ou ajout.

-   **Qualité globale (OverallQual)** : Une évaluation de la qualité globale des matériaux et de la finition de la maison.

-   **Condition globale (OverallCond)** : Une évaluation de l'état général de la maison.

-   **Nombre de garages (GarageCars)** : Le nombre de places de garage.

-   **Surface du garage (GarageArea)** : La taille du garage en pieds carrés.

-   **Sous-sol (TotalBsmtSF)** : La surface totale du sous-sol.

### 2. **Caractéristiques du Terrain :**

-   **Surface du terrain (LotArea)** : La taille du terrain en pieds carrés.

-   **Forme du terrain (LotShape)** : La forme du terrain (régulière, irrégulière, etc.).

-   **Type de zonage (Zoning)** : Le type de zonage résidentiel.

### 3. **Emplacement :**

-   **Quartier (Neighborhood)** : Le quartier où se trouve la maison.

-   **Proximité des services (Condition1, Condition2)** : La proximité des divers services et commodités (écoles, parcs, centres commerciaux, etc.).

-   **Qualité des écoles locales** : La qualité des écoles dans la zone (souvent prise en compte par un score ou une évaluation).

### 4. **Caractéristiques Extérieures :**

-   **Type de façade (Exterior1st, Exterior2nd)** : Le type de matériau de façade extérieur.

-   **Matériaux de toiture (RoofMatl)** : Le matériau de la toiture.

-   **Type de fondation (Foundation)** : Le type de fondation (béton, bois, etc.).

### 5. **Caractéristiques Intérieures :**

-   **Type de chauffage (Heating)** : Le type de système de chauffage.

-   **Type de climatisation (CentralAir)** : Si la maison dispose de la climatisation centrale.

-   **Qualité de la cuisine (KitchenQual)** : Une évaluation de la qualité de la cuisine.

### 6. **Autres Variables :**

-   **Residual standard error** : représente l'écart type des résidus, une mesure de la dispersion des observations autour de la ligne de régression.

-   **Multiple R-squared** : variabilité expliquée

-   Adjusted R-squared : Ajusté pour le nombre de variables explicatives dans le modèle.

-   F-statistic : teste l'hypothèse nulle que tous les coefficients sont égaux à zéro (sauf l'intercept). La p-value associée (\<2.2e-16) montre que le modèle dans son ensemble est significatif.

# métrique d'évaluation (modèle le plus simple)

```{r}


files <- data.frame(
  Id = test$Id,
  SalePrice = rep(mean(train$SalePrice), nrow(test))
  )

# write.csv(data, file = "output.csv", row.names = FALSE)

files %>% head(6)
```

le modèle le plus simple (modèle à 1 paramètre) a comme score **0.42577**

on regroupe les données de train et test pour éffectuer les mêmes modifications

# strees

```{r}


## trop de données manquantes
data <- data %>% 
    dplyr::select( -c(MiscVal, PoolArea)) 


## variable avec une modalité dominante dépassant 97%

 data <- data %>% 
  dplyr::select( -c(Street, Utilities, Condition2, RoofMatl, Heating))

```

la colonne `Id` n’est pas nécessaire pour la formation du modèle.

### Id, Yr

```{r}
data <- data %>%  dplyr::select( - c(Id, YrSold) )
```

## MSSubClass

``` r
data <- data %>% mutate(
        MSSubClass = ifelse(MSSubClass == 150, mode(MSSubClass), MSSubClass ),
        MSZoning = ifelse(MSSubClass == NA, mode(MSZoning), MSZoning ),
        Exterior1st = ifelse(Exterior1st == NA, mode(Exterior1st), Exterior1st ),
        Exterior2nd = ifelse(Exterior2nd == NA, mode(Exterior2nd), Exterior2nd )
          
)
```

```{r}
data <- data %>% mutate(
        MSSubClass = ifelse(MSSubClass == 150, NA, MSSubClass ),
        MSZoning = ifelse( is.na(MSSubClass), NA, MSZoning ),
        Exterior1st = ifelse( is.na(Exterior1st), NA, Exterior1st ),
        Exterior2nd = ifelse( is.na(Exterior2nd), NA, Exterior2nd )
          
)
```

# MasVnrArea masvarty

```{r}
unique(data$MasVnrType)
```

```{r}

data <- data %>%
  mutate(MasVnrArea = case_when(
    is.na(MasVnrArea) ~ "None",
    MasVnrArea == 0 ~ "None",
    0 <= MasVnrArea & MasVnrArea <= 250 ~ "1",
    250 < MasVnrArea & MasVnrArea <= 500 ~ "2",
    500 < MasVnrArea  ~ ">500"
  ))
```

# traitement de données

```{r}
## Convertir les variables à étiquettes en chaîne de charactère

data$MSSubClass <- as.character(data$MSSubClass)
data$OverallQual <- as.character(data$OverallQual)
data$OverallCond <- as.character(data$OverallCond)

data$Fireplaces <- as.character(data$Fireplaces)
data$GarageCars <- as.character(data$GarageCars)
data$BsmtFullBath <- as.character(data$BsmtFullBath)
data$BsmtHalfBath <- as.character(data$BsmtHalfBath)

data$FullBath <- as.character(data$FullBath)

data$HalfBath <- as.character(data$HalfBath)
data$BedroomAbvGr <- as.character(data$BedroomAbvGr)
data$KitchenAbvGr <- as.character(data$KitchenAbvGr)


data$TotRmsAbvGrd <- as.character(data$TotRmsAbvGrd)


data$MoSold <- as.character(data$MoSold)

### création de nouvelles variables

# housse_age
data <- data %>%
  mutate(housse_age = 2011 - YearBuilt) %>%
  dplyr::select(-YearBuilt)

data <- data %>%
  mutate(housse_age = case_when(
    0 <= housse_age & housse_age <= 10 ~ "1",
    10 < housse_age & housse_age <= 20 ~ "2",
    20 < housse_age & housse_age <= 30 ~ "3",
    30 < housse_age & housse_age <= 40 ~ "4",
    40 < housse_age & housse_age <= 65 ~ "5",
    65 < housse_age & housse_age <= 80 ~ "6",
    80 < housse_age  ~ ">80"
  ))



# garage_age
data <- data %>%
  mutate(garage_age = 2011 - GarageYrBlt) %>%
  dplyr::select(-GarageYrBlt)

data <- data %>%
  mutate(garage_age = case_when(
    is.na(garage_age) ~ "No_Garage",
    0 <= garage_age & garage_age <= 10 ~ "1",
    10 < garage_age & garage_age <= 20 ~ "2",
    20 < garage_age & garage_age <= 30 ~ "3",
    30 < garage_age & garage_age <= 40 ~ "4",
    40 < garage_age & garage_age <= 50 ~ "5",
    50 < garage_age & garage_age <= 60 ~ "6",
    60 < garage_age  ~ ">60"
  ))


# remod_since
data <- data %>%
  mutate(remod_since = 2011 - YearRemodAdd) %>%
  dplyr::select(-YearRemodAdd)

data <- data %>%
  mutate(remod_since = case_when(
    0 <= remod_since & remod_since <= 10 ~ "1",
    10 <= remod_since & remod_since <= 20 ~ "1",
    20 < remod_since & remod_since <= 40 ~ "2",
    40 < remod_since & remod_since <= 60 ~ "3",
    60 < remod_since  ~ ">60"
  ))


```

## discretisation des variables numériques

``` r

# GarageArea
data <- data %>%
  mutate(GarageArea = case_when(
    is.na(GarageArea) ~ "No.Garage",
    0 <= GarageArea & GarageArea <= 400 ~ "1",
    400 < GarageArea & GarageArea <= 600 ~ "2",
    600 < GarageArea & GarageArea <= 800 ~ "3",
    800 < GarageArea  ~ ">800"
  ))


# X1stFlrSF

data <- data %>%
  mutate(X1stFlrSF = case_when(
    is.na(X1stFlrSF) ~ "No.Garage",
    0 <= X1stFlrSF & X1stFlrSF <= 750 ~ "1",
    750 < X1stFlrSF & X1stFlrSF <= 1000 ~ "2",
    1000 < X1stFlrSF & X1stFlrSF <= 1250 ~ "3",
    1250 < X1stFlrSF & X1stFlrSF <= 1500 ~ "4",
    1500 < X1stFlrSF & X1stFlrSF <= 2000 ~ "5",
    2000 < X1stFlrSF  ~ ">2000"
  ))




# TotalBsmtSF
data <- data %>%
  mutate(TotalBsmtSF = case_when(
    TotalBsmtSF == 0 ~ "No.Basement",
    0 < TotalBsmtSF & TotalBsmtSF <= 300 ~ "1",
    300 <= TotalBsmtSF & TotalBsmtSF <= 600 ~ "2",
    600 < TotalBsmtSF & TotalBsmtSF <= 900 ~ "3",
    900 < TotalBsmtSF & TotalBsmtSF <= 1200 ~ "4",
    1200 < TotalBsmtSF & TotalBsmtSF <= 1500 ~ "5",
    1500 < TotalBsmtSF  ~ ">1500"
  ))




# BsmtFinSF1
data <- data %>%
  mutate(BsmtFinSF1 = case_when(
    BsmtFinSF1 == 0 ~ "No.Basement",
    0 < BsmtFinSF1 & BsmtFinSF1 <= 250 ~ "1",
    250 <= BsmtFinSF1 & BsmtFinSF1 <= 500 ~ "2",
    500 < BsmtFinSF1 & BsmtFinSF1 <= 750 ~ "3",
    750 < BsmtFinSF1 & BsmtFinSF1 <= 1000 ~ "4",
    1000 < BsmtFinSF1 & BsmtFinSF1 <= 1500 ~ "5",
    1500 < BsmtFinSF1  ~ ">1500"
  ))




# BsmtUnfSF
data <- data %>%
  mutate(BsmtUnfSF = case_when(
    BsmtUnfSF == 0 ~ "No.Basement",
    0 < BsmtUnfSF & BsmtUnfSF <= 500 ~ "1",
    500 < BsmtUnfSF & BsmtUnfSF <= 1000 ~ "2",
    1000 < BsmtUnfSF   ~ ">1000"
  ))







# GrLivArea
data <- data %>%
  mutate(GrLivArea = case_when(
    0 <= GrLivArea & GrLivArea <= 700 ~ "1",
    700 < GrLivArea & GrLivArea <= 1000 ~ "2",
    1000 < GrLivArea & GrLivArea <= 1300 ~ "3",
    1300 < GrLivArea & GrLivArea <= 1600 ~ "4",
    1600 < GrLivArea & GrLivArea <= 1800 ~ "5",
    1800 < GrLivArea & GrLivArea <= 2100 ~ "6",
    2100 < GrLivArea & GrLivArea <= 2800 ~ "7",
    2800 < GrLivArea  ~ ">2800",
  ))




# X2ndFlrSF
data <- data %>%
  mutate(X2ndFlrSF = case_when(
    X2ndFlrSF == 0 ~ "No_2nd_floor",
    0 < X2ndFlrSF & X2ndFlrSF <= 500 ~ "0-500",
    500 < X2ndFlrSF & X2ndFlrSF <= 1000 ~ "500-1000",
    1000 < X2ndFlrSF & X2ndFlrSF <= 1500 ~ "1000-1500",
    1500 < X2ndFlrSF  ~ ">1500",
  ))




# WoodDeckSF
data <- data %>%
  mutate(WoodDeckSF = case_when(
    WoodDeckSF == 0 ~ "No_WoodDeckSF",
    0 < WoodDeckSF & WoodDeckSF <= 200 ~ "0-200",
    200 < WoodDeckSF  ~ ">200",
  ))
```

### encore une discrétisation de variable numérique \[Yes, No \]

```{r}
data <- data %>%
  mutate( OpenPorchSF = ifelse(OpenPorchSF == 0, "No", "Yes"),
          EnclosedPorch = ifelse(EnclosedPorch == 0, "No", "Yes"),
          X3SsnPorch =  ifelse(X3SsnPorch == 0, "No", "Yes"),
          ScreenPorch = ifelse(ScreenPorch == 0, "No", "Yes"),
          LowQualFinSF = ifelse(LowQualFinSF == 0, "No", "Yes"),
          BsmtFinSF2 = ifelse(BsmtFinSF2 == 0, "No", "Yes")
          )
```

## Mising value

```{r}
#| layout-nrow: 2
#| layout-ncol: 2


missing_loliplot(data)

missing_variables(data)
```

```{r}
#| layout-nrow: 2
#| layout-ncol: 2


# gg_miss_case(train)
hist(n_miss_row(data), xlab = "", main =  "Histograme du nombre de valeurs manquantes par ligne")

missing_heatmap(data)
```

#### certaines NA ont une raisons connu d'être

```{r}
data <- data %>%
  mutate(
    Alley = ifelse(is.na(Alley), "No.alley.access", Alley),
    
    BsmtQual = ifelse(is.na(BsmtQual), "No.Basement", BsmtQual),
    BsmtCond = ifelse(is.na(BsmtCond), "No.Basement", BsmtCond),
    BsmtExposure = ifelse(is.na(BsmtExposure), "No.Basement", BsmtExposure),
    BsmtFinType1 = ifelse(is.na(BsmtFinType1), "No.Basement", BsmtFinType1),
    BsmtFinType2 = ifelse(is.na(BsmtFinType2), "No.Basement", BsmtFinType2),
    
    MiscFeature = ifelse(is.na(MiscFeature), "None", "Yes"),
    Fence = ifelse(is.na(Fence), "None", Fence),
    FireplaceQu = ifelse(is.na(FireplaceQu), "No.Fireplace", FireplaceQu),
    
    GarageType = ifelse(is.na(GarageType), "No.Garage", GarageType),
    GarageFinish = ifelse(is.na(GarageFinish), "No.Garage", GarageFinish),
    GarageQual = ifelse(is.na(GarageQual), "No.Garage", GarageQual),
    GarageCond = ifelse(is.na(GarageCond), "No.Garage", GarageCond),

    PoolQC = ifelse(is.na(PoolQC), "No.Pool", "Yes")
  )%>%
  dplyr::select(- PoolQC)
```

```{r}
missing_heatmap(data)
```

#### Missforest for other NA input

Missforest permet d'imputer les valeurs manquantes dans les jeux de données de type **mixte** en utilisant les **forêts aléatoires** pour prédire les valeurs manquantes de manière itérative et non paramétrique, en tirant parti des relations entre les variables.

missForest surpasse les autres méthodes d'imputation, en particulier dans les contextes de données où des interactions complexes et des relations non linéaires sont suspectées.

![](images/clipboard-4011442430.png)

je prépare mes données pour l'algorithme

```{r}
# Transformer toutes les colonnes de type caractère en facteur
data <- data %>%
  mutate(across(where(is.character), as.factor))
         
var_type_viz_1(data)

```

```{r}
# Stocker les types initiaux pour chaque ligne
# types_lines <- lapply(data, function(line) sapply(line, class))

library(missForest)
data.imp <- missForest(data, verbose = TRUE, maxiter = 4, ntree = 50, replace = FALSE)
```

```{r}
colSums(is.na(data.imp$ximp))
```

## on enlève les modalités des variables cateégorielles et on les rempplace par des valeur manquantes qu'on va inputer à nouveau par la methode de miss forest.

```{r}
data <- data.imp$ximp
sum(is.na(data))
```

```{r}
data <- data %>%
  mutate(across(where(is.factor), as.character))

cat_data <- data %>% select_if(is.character)

cat_vars <- cat_data %>% colnames()

for (i in cat_vars) {
      data1 <- data[1:nrow(train), ]
      levels1 <- data1[[i]] %>% unique() %>% as.character()
      
      data2 <-  data[(nrow(train) + 1):nrow(data) , ]
      
      levels2 <- data2[[i]] %>% unique() %>% as.character()
      
      # Identifier les modalités communes
      common_levels <- intersect(levels1, levels2)
      factor <- data[[i]] %>% as.character()
      data[[i]] <- ifelse( !factor %in% common_levels, NA, factor   ) 
}

 
```

```{r}
sum(is.na(data))
```

```{r}
missing_loliplot(data)
```

```{r}
data$MSSubClass <- as.character(data$MSSubClass)
data$OverallQual <- as.character(data$OverallQual)
data$OverallCond <- as.character(data$OverallCond)

data$Fireplaces <- as.character(data$Fireplaces)
data$GarageCars <- as.character(data$GarageCars)
data$BsmtFullBath <- as.character(data$BsmtFullBath)
data$BsmtHalfBath <- as.character(data$BsmtHalfBath)

data$FullBath <- as.character(data$FullBath)

data$HalfBath <- as.character(data$HalfBath)
data$BedroomAbvGr <- as.character(data$BedroomAbvGr)
data$KitchenAbvGr <- as.character(data$KitchenAbvGr)


data$TotRmsAbvGrd <- as.character(data$TotRmsAbvGrd)

data$MoSold <- as.character(data$MoSold)

data <- data %>%
  mutate(across(where(is.integer), as.numeric)) %>%
  mutate(across(where(is.character), as.factor))

var_type_viz_1(data)
```

```{r}
data <- data %>%
  dplyr::select(-type)

data.imp <- missForest(data, verbose = TRUE, maxiter = 4, ntree = 50, replace = FALSE)
```

```{r}
sum(is.na(data.imp$ximp))
```

```{r}
write.csv(data.imp$ximp, file = "inputed_data.csv", row.names = FALSE)
```

# \>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>\>

```{r}
data <- read.csv("inputed_data.csv", na.strings = "NA")
sum(is.na(data))
```

# colinéarité

## Matrice de corrélation

```{r}
library(dplyr)
library(igraph)
data$MSSubClass <- as.character(data$MSSubClass)
data$OverallQual <- as.character(data$OverallQual)
data$OverallCond <- as.character(data$OverallCond)

data$Fireplaces <- as.character(data$Fireplaces)
data$GarageCars <- as.character(data$GarageCars)
data$BsmtFullBath <- as.character(data$BsmtFullBath)
data$BsmtHalfBath <- as.character(data$BsmtHalfBath)

data$FullBath <- as.character(data$FullBath)

data$HalfBath <- as.character(data$HalfBath)
data$BedroomAbvGr <- as.character(data$BedroomAbvGr)
data$KitchenAbvGr <- as.character(data$KitchenAbvGr)


data$TotRmsAbvGrd <- as.character(data$TotRmsAbvGrd)

data$MoSold <- as.character(data$MoSold)


```

```{r}
library(dplyr)
library(ggplot2)
library(rlang)

cat_selection <- data[1:nrow(train), ]
cat_selection$SalePrice <- label

R2_coef <- function(y_pred, y){
  sst <- sum((y - mean(y))^2)
  sse <- sum((y_pred - y)^2)
  rsq <- 1 - sse/sst
  rsq
}

cat_feature_importance <- function(data, label){
  
  var_name <- as_label(enquo(label))
  
  cat_data <- data %>%
    mutate(across(where(is.character), as.factor)) %>%
    dplyr::select(where(is.factor))
  
  cat_vars <- colnames(cat_data)
  #cat_vars <- cat_vars[1:3]  # Adjust the number of variables to be considered
  n <- nrow(data)
  
  R2 <- c()
  for (var in cat_vars) {
    
    grouped_data <- data %>%
      group_by(!!sym(var)) %>%
      summarise(mean_SalePrice = mean(!!sym(var_name), na.rm = TRUE))  # Calculate mean SalePrice for each group
    
    y <- rep(0, times = n)
    
    for (i in 1:n){
      ind <- which(grouped_data[[1]] == data[[var]][i])
      y[i] <- grouped_data$mean_SalePrice[ind]
    }
    R2 <- c(R2, R2_coef(y, data[[var_name]]))
    
  }
  
  df <- data.frame(variables = cat_vars, R2 = R2)
  
  df %>%
    arrange(desc(R2)) %>%  # Arrange by R² in descending order
    mutate(variables = factor(variables, levels = rev(variables))) %>%  # Set the order of factors
    ggplot(aes(x = variables, y = R2)) +
    geom_segment(aes(xend = variables, yend = 0), color = "grey") +
    geom_point(size = 2, color = "blue") +
    coord_flip() +
    labs(title = "Variables catégorielles qui expliquent le mieux la variance de SalePrice",
         x = "",
         y = "R²") +
    theme_minimal()
}

cat_feature_importance(cat_selection, SalePrice)

```

```{r}
library(dplyr)
library(ggplot2)
library(rlang)

cat_selection <- data[1:nrow(train), ]
cat_selection$SalePrice <- label

lm_R2 <- function(y, x){
  model <- lm(y ~ x)
  rsq <- summary(model)$r.squared
  rsq
}

num_feature_importance <- function(data, label){
  
  var_name <- as_label(enquo(label))
  
  num_data <- data %>%
    dplyr::select(where(is.numeric))
  
  num_vars <- colnames(num_data)
  num_vars <- num_vars[num_vars != var_name]  # Exclure la variable cible
  n <- nrow(data)
  
  R2 <- c()
  for (var in num_vars) {
    x <- data[[var]]
    y <- data[[var_name]]
    rsq <- lm_R2(y, x)
    R2 <- c(R2, rsq)
  }
  
  df <- data.frame(variables = num_vars, R2 = R2)
  
  df %>%
    arrange(desc(R2)) %>%  # Ordonner par R² du plus grand au plus petit
    mutate(variables = factor(variables, levels = rev(variables))) %>%  # Définir l'ordre des facteurs
    ggplot(aes(x = variables, y = R2)) +
    geom_segment(aes(xend = variables, yend = 0), color = "grey") +
    geom_point(size = 2, color = "red") +
    coord_flip() +
    labs(title = "Variables numériques qui expliquent le mieux la variance de SalePrice",
         x = "",
         y = "R²") +
    theme_minimal()
}

num_feature_importance(cat_selection, SalePrice)


```

```{r}
tools <- numerical_corelation(data, 0.6)
tools
```

-   tools \<- numerical_corelation(data, 0.6) tools**%IncMSE** (Mean Decrease in Accuracy) : Mesure combien l'erreur du modèle augmente lorsqu'une variable est permutée. Plus la valeur est élevée, plus la variable est importante.

-   **IncNodePurity** (Mean Decrease in Gini) : Mesure la réduction totale de l'impureté de Gini obtenue en utilisant cette variable dans le modèle. Plus la valeur est élevée, plus la variable est importante.

```{r}
library(randomForest)
train_f <- function(data){
  cleaned_train <- data[1:nrow(train), ]
  cleaned_train$SalePrice = label
  
  cleaned_train
}

# i in 1 : n
i <- 1
n <- nrow(tools$data)

var <- strsplit( tools$data[1,2] %>% pull, "\\s+")[[1]]
var <- gsub(",$", "", var)
var <- c(var, "SalePrice")
df <- train_f(data) %>% dplyr::select(var)

model <- randomForest(SalePrice ~ ., data = df, importance = TRUE)

viz_rf_model_2(model)

```

```{r}
data <- data %>% dplyr::select(-c(X1stFlrSF, X2ndFlrSF))
```

```{r}
data <- data %>% dplyr::select(-c(TotalBsmtSF,	LotFrontage, 	BsmtFinSF1, BsmtUnfSF, GarageArea))
```

## Corrélations V de Cramer :

```{r}
# Fonction pour créer la matrice des p-values
# create_chi2_pvalue_matrix(data)
```

le V de Cramer est basé sur la statistique du chi-carré et varie de 0 à 1, où 0 indique aucune association et 1 indique une association parfaite.

```{r}
tools <- categorical_corelation(data, 0.65)
tools
```

```{r}
library(randomForest)
train_f <- function(data){
  cleaned_train <- data[1:nrow(train), ]
  cleaned_train$SalePrice = label
  
  cleaned_train
}

# i in 1 : n
i <- 1
n <- nrow(tools$data)

var <- strsplit( tools$data[1,2] %>% pull, "\\s+")[[1]]
var <- gsub(",$", "", var)
var <- c(var, "SalePrice")
df <- train_f(data) %>% select(var)

model <- randomForest(SalePrice ~ ., data = df, importance = TRUE)

viz_rf_model_2(model)

```

```{r}
data <- data %>% dplyr::select(-c(GarageType, BldgType, HouseStyle, MSZoning, ExterQual, Exterior2nd, BsmtFinSF2, GarageCond, garage_age))
```

## Analyse de la variance inflation factor (VIF)

```{r}


cleaned_train <- data[1:nrow(train), ]

cleaned_train$SalePrice = label

cleaned_test <- data[(nrow(train) + 1):nrow(data), ]


```

```{r}
dim(cleaned_train)
dim(train)
```

```{r}
dim(cleaned_test)
dim(test)
```

```{r}
cleaned_train <- cleaned_train %>%
  mutate(across(where(is.integer), as.numeric))


var_type_viz_1(cleaned_train)
```

```{r}
df1 <- cleaned_train %>% dplyr::select(where(is.numeric ))

cat_vars <- df1 %>% colnames()

for (i in cat_vars) {
  Q1 <- quantile(df1[[i]], 0.25)
  Q3 <- quantile(df1[[i]], 0.75)
  IQR <- Q3 - Q1
  
  cleaned_train <- cleaned_train[cleaned_train[[i]] <= (Q3 + 1.5 * IQR), ]
  cleaned_train <-cleaned_train[cleaned_train[[i]] >= (Q1 - 1.5 * IQR), ]
  cleaned_train <- cleaned_train[complete.cases(cleaned_train), ]  # Supprimer les lignes avec des valeurs manquantes
  cleaned_train <- droplevels(cleaned_train)  # Supprimer les niveaux de facteur inutilisés
}

# Affichage des premières lignes du dataframe df1
print(head(df1))

# Affichage de l'inference
cat(sprintf("\n\033[1mInference:\033[0m\nBefore removal of outliers, The dataset had %d samples.\n", nrow(df1)))
cat(sprintf("After removal of outliers, The dataset now has %d samples.\n", nrow(cleaned_train)))

```

## One-Hot Encoding

convertit chaque catégorie unique en une colonne binaire.

-   *remove_first_dummy = TRUE*: éviter la multicolinéarité

-   *remove_selected_columns = TRUE*: Supprime les colonnes originales après l'encodage one-hot.

```{r}
data <- data %>% dplyr::select(-c( BedroomAbvGr, LowQualFinSF, Electrical, HeatingQC, Foundation, ExterCond, MasVnrType , RoofStyle, LandSlope, LotShape, Alley  ))
```

```{r}
library(fastDummies)  

data_onehot <- dummy_cols(data, remove_first_dummy = TRUE, remove_selected_columns = TRUE)
```

```{r}

tools <- numerical_corelation(data_onehot, 0.80)
tools$data
```

```{r}
library(randomForest)
train_f <- function(data){
  cleaned_train <- data[1:nrow(train), ]
  cleaned_train$SalePrice = label
  
  cleaned_train
}

# i in 1 : n
i <- 1
n <- nrow(tools$data)

var <- strsplit( tools$data[4,2] %>% pull, "\\s+")[[1]]
var <- gsub(",$", "", var)
var <- c(var, "SalePrice")
df <- train_f(data_onehot) %>% dplyr::select(var)

model <- randomForest(SalePrice ~ ., data = df, importance = TRUE)

viz_rf_model_2(model)

```

```{r}
data_onehot <- data_onehot %>%
  dplyr::select(-c( BsmtQual_No.Basement, BsmtCond_No.Basement, BsmtExposure_No.Basement, BsmtFinType1_No.Basement, KitchenAbvGr_2, GarageFinish_No.Garage, SaleCondition_Partial))
```

```{r}
data_onehot <- data_onehot %>%
  dplyr::select(-c(KitchenAbvGr_1))
```

```{r}
nrow(data_onehot[1:nrow(train), ])
```

```{r}


```

# Linear Regression GLM

```{r}
oh_cleaned_train <- data_onehot[1:nrow(train), ]
oh_cleaned_train$SalePrice = label

oh_cleaned_test <- data_onehot[(nrow(train) + 1):nrow(data_onehot), ]
```

```{r}

modele_1 = lm(SalePrice ~ ., data = oh_cleaned_train)
predict(modele_1, oh_cleaned_test) %>% as.data.frame() %>%head()


residuals <- modele_1$residuals
```

## Relation linéaire

l'intensité de la relation linéaire entre la variable indépendante, x, et la variable dépendante, y est mesurée par le nombre d'étoile

```{r}
summary(modele_1)

par(mfrow=c(2,2)) 
plot(modele_1) 
```

## Indépendance

Les résidus sont indépendants. En particulier, il n’y a pas de corrélation entre les résidus consécutifs dans les données de séries chronologiques.

```{r}
# Produire le graphique d'autocorrélation des résidus
acf(residuals, main = "Graphique d'autocorrélation des résidus")
```

Test de Durbin-Watson :

-   Une valeur proche de 2 indique une absence d'autocorrélation.

-   Une valeur beaucoup plus basse que 2 suggère une autocorrélation positive.

-   Une valeur beaucoup plus élevée que 2 suggère une autocorrélation négative.

```{r}
# Effectuer le test de Durbin-Watson
dw_test <- dwtest(modele_1)

# Afficher les résultats du test de Durbin-Watson
print(dw_test)
```

## Homoscédasticité : Les résidus ont une variance constante à chaque niveau de x.

```{r}
plot.res(modele_1)
```

-   Si le graphique montre que les résidus sont répartis de manière aléatoire autour de zéro, sans motif clair , cela suggère que les résidus sont homoscédastiques.

-   Si vous voyez un motif (par exemple, les résidus augmentent ou diminuent systématiquement avec les valeurs ajustées), cela suggère une hétéroscédasticité.

**Test de Breusch-Pagan** : Ce test statistique vérifie formellement l'homoscédasticité des résidus.

```{r}
library(lmtest)

# Effectuer le test de Breusch-Pagan
bp_test <- bptest(modele_1)

# Afficher les résultats du test de Breusch-Pagan
print(bp_test)
```

-   Si la valeur p est inférieure à un seuil (typiquement 0,05), je peux rejeter l'hypothèse nulle d'homoscédasticité, ce qui suggère une hétéroscédasticité.

-   Si la valeur p est supérieure au seuil, vous ne pouvez pas rejeter l'hypothèse nulle, ce qui suggère que les résidus sont homoscédastiques.

## Normalité : Les résidus du modèle sont normalement distribués.

test de **Shapiro-Wilk**, qui est couramment utilisé pour évaluer la normalité des données.

```{r}

# Effectuer le test de Shapiro-Wilk pour tester la normalité des résidus
shapiro_test <- shapiro.test(residuals)

# Afficher les résultats du test de Shapiro-Wilk
print(shapiro_test)
```

-   Si la valeur p est inférieure à un seuil (typiquement 0,05), on rejete l'hypothèse nulle selon laquelle les résidus suivent une distribution normale.

-   Si la valeur p est supérieure au seuil, on ne peut pas rejeter l'hypothèse nulle, ce qui suggère que les résidus peuvent être considérés comme suivant une distribution normale.

```{r}
files <- data.frame(
  Id = test$Id,
  SalePrice = predict(modele_1, cleaned_test)
)
rownames(files) <- NULL
files %>% head(10) %>% gt_trabi()

write.csv(files, file = "output.csv", row.names = FALSE)

```

# Stepwise variable selection

```{r}
```

```{r}
# Charger les packages nécessaires
library(stats)      # Contient lm, add1, update, drop1
library(ggfortify)
```

```{r}
M0 <- lm(SalePrice ~ 1, data = oh_cleaned_train) # null model
M1 <- lm(SalePrice ~ ., data = oh_cleaned_train) # full model
summary(M1)
```
```{r}
autoplot(M1, which=1, ncol=1)
```

```{r}
autoplot(M1, which=4, ncol=1)
```


```{r}
#step_model <- add1(M0, scope = Mf, data = oh_cleaned_train, test = "F")
#step_model

```

-   `add1` est utilisé pour évaluer l'ajout de chaque variable explicative possible à partir du modèle nul `M1` en considérant le modèle complet `Mf` comme portée (`scope`).

```{r}
#M1 <- update(M0, . ~ .+ LotArea + GrLivArea + WoodDeckSF, data = oh_cleaned_train)
#autoplot(M2, which=1, ncol=1)
```
Forward selection
Usually start from the null model.
Every variable outside the current model <none> can be added (+) one at time at each step until AIC is no better (<none> has the lowest AIC in the top of the list).
```{r}
# Forward using AIC
# M1.forw <- step(M0, scope = list(lower = M0,  upper = M1), direction = "forward", k = 2)
```

```{r}
summary(M1.forw)
```

```{r}
extractAIC(M1.forw)
```
Stepwise selection
Start from any model such as the full model.
Every variable outside the current model <none> can be added (+) and those inside can be dropped (-) one at time at each step until AIC is no better.
```{r}
M1 <- lm(SalePrice ~ ., data = oh_cleaned_train) # full model
M1.step <- step(M1, scope = list(lower = SalePrice ~ 1,  upper = SalePrice ~ .),direction = "both", k = 2)
```

```{r}
extractAIC(M1.step)
```


# regularisation

la régularisation permet d'éviter le sur-ajustement en limitant la capacité d’apprentissage ou la flexibilité d’un modèle d’apprentissage machine.

#### La régression lasso est une méthode que nous pouvons utiliser pour ajuster un modèle de régression lorsque la multicollinéarité est présente dans les données.

la régression des moindres carrés tente de trouver des estimations de coefficients qui minimisent la somme des résidus au carré (RSS) :

**RSS = Σ(y~J~ – ŷ~J~)2**

À l’inverse, la régression lasso cherche à minimiser les éléments suivants :

**RSS + λΣ\|β~j~\|**

où *j* va de 1 à *p* variables prédictives et λ ≥ 0.

Ce deuxième terme de l’équation est connu sous le nom de *pénalité de rétrécissement*. Dans la régression lasso, nous sélectionnons une valeur pour λ qui produit l’erreur quadratique moyenne la plus faible possible.

### **Étape 2 : Ajuster le modèle de régression au lasso**

Ensuite, nous utiliserons la fonction **glmnet()** pour ajuster le modèle de régression lasso et spécifier **alpha=1**.

-   \alpha = 0 équivaut à utiliser la régression de ridge

-   \$ 0\< \alpha \< 1\$ à une valeur comprise entre 0 et 1 équivaut à utiliser un elactic net.

Pour déterminer la valeur à utiliser pour lambda, nous allons effectuer une [validation croisée k-fold](https://www.statology.org/k-fold-cross-validation/) et identifier la valeur lambda qui produit l’erreur quadratique moyenne (MSE) la plus faible.

Notez que la fonction **cv.glmnet()** effectue automatiquement une validation croisée k-fold en utilisant k = 10 folds.

5.7 Librairie glmnet

L’utilisation de la librairie glmnet fournit des résultats plus rapides, ce qui peut s’avérer important pour des données de grande dimension.

Par contre, on ne peut pas traiter à priori des variables qualitatives. Nous allons donc devoir créer des vecteurs avec des variables indicatrices des diverses modalités pour les variables qualitatives .

```{r}
alpha = 1


lasso_label <- label
lasso_train <- as.matrix(data_onehot[1:nrow(train), ])
lasso_test <- as.matrix(data_onehot[(nrow(train) + 1):nrow(data_onehot), ])

library(glmnet)

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(lasso_train, lasso_label, alpha = alpha )

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

#produce plot of test MSE by lambda value
plot(cv_model)
```

La valeur lambda qui minimise l’ESM de test s’avère être \`r best_lambda\`

### **Étape 3 : Analyser le modèle final**

Enfin, nous pouvons analyser le modèle final produit par la valeur lambda optimale.

Nous pouvons utiliser le code suivant pour obtenir les estimations de coefficient pour ce modèle :

```{r}
#find coefficients of best model
best_model <- glmnet(lasso_train, lasso_label, alpha = alpha, lambda = best_lambda)
coef(best_model)
```

Aucun coefficient n’est affiché pour le **prédicteur remod_since_3** car la régression lasso a réduit le coefficient à zéro. Cela signifie qu’il a été complètement retiré du modèle parce qu’il n’était pas assez influent.

La régression de ridge réduit tous les coefficients *vers* zéro, mais la régression lasso a le potentiel de supprimer les prédicteurs du modèle en réduisant *complètement* les coefficients à zéro.

Nous pouvons également utiliser le modèle de régression lasso final pour faire des prédictions sur de nouvelles observations

nous pouvons calculer le R au carré du modèle sur les données d’entraînement 

```{r}
#use fitted best model to make predictions
R2_coef <- function(y_pred, y){
  #find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_pred - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
  
}



y_predicted <- predict(best_model, s = best_lambda, newx = lasso_train)

R2_coef(y_predicted, lasso_label)
```

```{r}
y_predicted <- predict(best_model, s = best_lambda, newx = lasso_test)
files <- data.frame(
  Id = test$Id,
  SalePrice = y_predicted
)
rownames(files) <- NULL

names(files) <- c("Id", "SalePrice")

files %>% head(10) %>% gt_trabi()

write.csv(files, file = "output.csv", row.names = FALSE)

```

# random forest

```{r}
# Ajuster le modèle de forêt aléatoire
model <- randomForest(SalePrice ~ ., data = cleaned_train, importance = TRUE)
viz_rf_model_2(model)
```

```{r}
library(caret)
library(randomForest)


# Définir les paramètres de la validation croisée
control <- trainControl(method = "cv",
                        number = 5)

# Ajuster le modèle de forêt aléatoire avec la validation croisée
set.seed(123) # Pour la reproductibilité
model <- train(SalePrice ~ ., data = cleaned_train, method = "rf", trControl = control, importance = TRUE)

# Afficher les résultats
print(model)

# Importance des variables
varImpPlot(model$finalModel)

```

```{r}
 importance(model$finalModel) %>%
    data.frame() %>%
    rownames_to_column("term") %>%
  arrange(desc(X.IncMSE)) %>%  # Ordonner par X.IncMSE du plus grand au plus petit
  mutate(term = factor(term, levels = rev(term))) %>%  # Définir l'ordre des facteurs
  filter(X.IncMSE < 0) %>%
    ggplot(aes(x = term, y = X.IncMSE)) +
    geom_segment(aes(xend = term, yend = 0), color = "grey") +
    geom_point(size = 2, color = "blue") +
    coord_flip() +
    labs(title = "random forest fearture importance",
         x = "",
         y = "%IncMSE") +
    theme_minimal()
```

```{r}
files <- data.frame(
  Id = test$Id,
  SalePrice = predict(model, cleaned_test)
)
rownames(files) <- NULL
files %>% head(10) %>% gt_trabi()

write.csv(files, file = "output.csv", row.names = FALSE)

```

```{r}
modlin = lm(SalePrice ~ ., data = cleaned_train)
summary(modlin) # noter les p-valeurs



plot.res=function(x,y,titre="")
{
plot(x,y,col="blue",ylab="Résidus",
xlab="Valeurs predites",main=titre)
abline(h=0, col="green")
}



res=residuals(modlin)
#Regroupement des graphiques sur la meme page
par(mfrow=c(1,2))

hist(residuals(modlin))


qqnorm(res)
qqline(res, col = 2)


# retour au graphique standard
par(mfrow=c(1,1))
plot.res(predict(modlin),res)
```

## Sélection de modèle par sélection de variables

# Sélection par AIC et backward

Akaike Information Criterion (AIC) L'AIC est un critère utilisé pour comparer différents modèles de régression et choisir celui qui offre le meilleur équilibre entre qualité d'ajustement et complexité du modèle. Il est défini comme suit :

$$ AIC = 2k - 2ln(L)$$

où : - $k$ est le nombre de paramètres du modèle. - $k$ est la vraisemblance maximale du modèle.

Utilisation de l'AIC dans les modèles linéaires Lors de l'ajustement de modèles linéaires, l'AIC peut être utilisé pour sélectionner les variables explicatives qui rendent le modèle le plus efficace. Un modèle avec un AIC plus bas est préféré, car il suggère un bon compromis entre une faible erreur de prédiction et une faible complexité.

```{r}
library(MASS)

modselect_b <- stepAIC(modlin2, ~. , trace=TRUE, direction = c("backward")) 

summary(modselect_b)
```

```{r}
train %>% distinct(FullBath)


test %>% distinct(FullBath)
```

```{r}
output <- data.frame(
  Id = Id,
  SalePrice = predict(modselect_b, test)
  )

write.csv(output, file = "data_AIC.csv", row.names = FALSE)

```

# Méthodes de sélection de modèles

Pour sélectionner automatiquement la meilleure combinaison de variables prédictives pour construire un modèle prédictif optimal. Il s’agit notamment des meilleures méthodes de sélection de sous-ensembles, de la régression par étapes et de la régression pénalisée (modèles de régression en crête, en lasso et en filet élastique). Nous présentons également des méthodes de régression basées sur les composantes principales, qui sont utiles lorsque les données contiennent plusieurs variables prédictives corrélées.

# (repeated) k-fold cross-validation

La méthode de validation croisée k-fold évalue la performance du modèle sur différents sous-ensembles de données d’entraînement, puis calcule le taux d’erreur de prédiction moyen.

L’algorithme est le suivant :

1.  Diviser aléatoirement l’ensemble de données en k-sous-ensembles (ou k-fold) (par exemple 5 sous-ensembles)

2.  Réserver un sous-ensemble et entraîner le modèle sur tous les autres sous-ensembles

3.  Tester le modèle sur le sous-ensemble réservé et enregistrer l’erreur de prédiction

4.  Répéter ce processus jusqu’à chacun des k

K-fold cross-validation (CV) is a robust method for estimating the accuracy of a model.

# Modèle linéaire

```{r}
library(caret)


# Define training control
set.seed(123) 

train.control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3 )
# Train the model
lm_model <- train(SalePrice ~., data = cleaned_train, method = "lm",
               trControl = train.control)

# Summarize the results
print(lm_model)

print(lm_model$yLimits)
```

## glm

```{r}
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3 )

# Ajuster le modèle glm avec validation croisée
glm_model <- train(SalePrice ~ ., data = train, method = "glm", family = "gaussian", trControl = train_control)

print(glm_model)
```

```{r}
plot( fitted(glm_model), residuals(glm_model) )
```

Le réglage des hyperparamètres est une étape critique du machine learning. Il s’agit de trouver le meilleur ensemble d’hyperparamètres qui maximisent les performances du modèle.

Les hyperparamètres sont des paramètres de configuration utilisés pour contrôler le processus d’entraînement d’un modèle.

Différents hyperparamètres peuvent avoir un impact significatif sur les performances de votre modèle. Le réglage de ces paramètres permet de:

Améliorer la précision du modèle. Réduire le sur-ajustement. Optimisez le temps de formation du modèle.

```{r}
install.packages("caret")
install.packages("xgboost")
install.packages("e1071")  # Required for caret's train function

library(e1071)
library(caret)
library(xgboost)

# Load your data
# Assume dtrain is already created using xgb.DMatrix

# Define the parameter grid
grid <- expand.grid(
  nrounds = 100,
  eta = c( 0.2, 0.3),
  max_depth = c(10, 12, 14),
  gamma = c(0,  0.2),
  colsample_bytree = c(0.7, 1.0),
  min_child_weight = c( 5, 7),
  subsample = c(0.6, 0.8)
)

# Set up training control
train_control <- trainControl(
  method = "cv",           # Use cross-validation
  number = 5,              # Number of folds in cross-validation
  verboseIter = TRUE,      # Print training log
  allowParallel = TRUE     # Allow parallel computation
)

# Train the model using caret
xgb_train <- train(
  x = as.matrix(train),    # Your training data (features)
  y = label,             # Your training labels
  trControl = train_control,
  tuneGrid = grid,
  method = "xgbTree",
  verbose = TRUE
)

# Print the best parameters found
print(xgb_train$bestTune)

# Train the final model with the best parameters
final_params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = xgb_train$bestTune$eta,
  max_depth = xgb_train$bestTune$max_depth,
  gamma = xgb_train$bestTune$gamma,
  colsample_bytree = xgb_train$bestTune$colsample_bytree,
  min_child_weight = xgb_train$bestTune$min_child_weight,
  subsample = xgb_train$bestTune$subsample
)

# Train the XGBoost model with the best parameters
xgb_model <- xgb.train(params = final_params, dtrain, nrounds = 100)

# Evaluate the model
preds <- predict(xgb_model, dtest)

```

```{r}
#| eval: false


library(devtools)
install_github("DillonHammill/DataEditR")

# Load required packages
library(DataEditR)

# Save output to R object & csv file
mtcars_new <- data_edit(iris,
                        save_as = "mtcars_new.csv")
```

```{r}
plot(model)
```

# 

La marge d'erreur (E) est donnée par : $$ E = t_{\alpha/2} \cdot \frac{s}{\sqrt{n}}$$

```{r}
intervalle_confiance_GLM <- function(model, confidence_level = 0.95 ){
  
coeficients <- summary(model)$coefficients[, 1]
ecart_type <- summary(model)$coefficients[, 2]


# 1445 degrees of freedom
n <- length( model$residuals)
p <- length(coefficients(model))
df <- n - p


a <- confidence_level

# Valeur critique de la t-distribution pour 95% de confiance
t_critical <- qt(c((1-a)/2, 1 - (1-a)/2), df = df)

lower_bound <- coeficients - t_critical[1] * ecart_type / sqrt(n)
upper_bound <- coeficients + t_critical[2] * ecart_type / sqrt(n)

intervalle_confiance <- cbind(lower_bound, upper_bound) %>%
  round(2) %>%
  as.data.frame() %>%
  rownames_to_column("parametres")
intervalle_confiance %>% gt_trabi()
  
  
}


intervalle_confiance_GLM(model = model, confidence_level = 0.95)
```

# Bagging et random forest

```{r}
result <- rforest(train, "SalePrice",  type = "regression", max.depth = 1)

cv.rforest(result, mtry = 1:3, min.node.size = seq(1, 10, 5))
```

```{r}
library(randomForest)


# Entraîner le modèle Random Forest
rf_model <- randomForest(SalePrice ~ ., data = train, ntree = 500, mtry = 10)

# Afficher un résumé du modèle
print(rf_model)
```

```{r}
output <- data.frame(
  Id = Id,
  SalePrice = predict(rf_model, test)
  )

write.csv(output, file = "data_rf.csv", row.names = FALSE)

```

# Boosting

```{r}
train <- data_onehot[1:nrow(train), ]
test <- data_onehot[(nrow(train) + 1):nrow(data), ]

# Construct xgb.DMatrix object from  local file.
dtrain <- xgb.DMatrix(data = as.matrix( train ), label = label, missing = NA)
dtest <- xgb.DMatrix(data = as.matrix( test ))


# Set XGBoost parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.3,
  max_depth = 6,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


# Train the XGBoost model
xgb_model <- xgb.train(params, dtrain, nrounds = 1000)

```

```{r}
output <- data.frame(
  Id = Id,
  SalePrice = predict(xgb_model, dtest)
  )

write.csv(output, file = "dataxgboost.csv", row.names = FALSE)

```

# random forest
